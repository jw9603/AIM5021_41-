{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOUpw6x0vCkWFWEBeXx1Fhj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jw9603/AIM5021_41-/blob/main/11thweek.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. RNN을 이용하여 텍스트 생성하기"
      ],
      "metadata": {
        "id": "cuCVlHd9bzf3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7ghf-5gZOOz",
        "outputId": "62a7711b-f21c-413e-fd50-1b86831452df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 데이터에 대한 이해와 전처리"
      ],
      "metadata": {
        "id": "AlIQr7Deb4UI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "JDMxylD6Z9GZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"경마장에 있는 말이 뛰고 있다\\n\n",
        "그의 말이 법이다\\n\n",
        "가는 말이 고와야 오는 말이 곱다\\n\"\"\""
      ],
      "metadata": {
        "id": "RnntwSZlaJGg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어 집합을 생성하고 크기를 확인해보자. 단어 집합의 크기를 저장할 때는 케라스 토크나이저의 정수 인코딩은 인덱스가 1부터 시작하지만, 패딩을 위한 0을 고려하여 +1을 해준다."
      ],
      "metadata": {
        "id": "PON2V659aNWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('단어 집합의 크기 : %d' % vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jcH5huAaMLN",
        "outputId": "33bb85a7-13b4-40a1-cd5d-6c234b460b0e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합의 크기 : 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "각 단어와 단어에 부여된 정수 인덱스를 출력해보자"
      ],
      "metadata": {
        "id": "r7bpzujCabI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZxKi4i_aZr6",
        "outputId": "210ff5a4-9ed0-4a95-ba67-8ac5c60c1b11"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'말이': 1, '경마장에': 2, '있는': 3, '뛰고': 4, '있다': 5, '그의': 6, '법이다': 7, '가는': 8, '고와야': 9, '오는': 10, '곱다': 11}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련 데이터를 만들어보자!"
      ],
      "metadata": {
        "id": "HdWuSrA5agzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = list()\n",
        "for line in text.split('\\n'): # 줄바꿈 문자를 기준으로 문장 토큰화\n",
        "    encoded = tokenizer.texts_to_sequences([line])[0]\n",
        "    print(encoded)\n",
        "    for i in range(1, len(encoded)):\n",
        "        sequence = encoded[:i+1]\n",
        "        sequences.append(sequence)\n",
        "\n",
        "print('학습에 사용할 샘플의 개수: %d' % len(sequences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1157vekae5i",
        "outputId": "59eb03c1-0994-4d61-f549-d03f86d8d67d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 1, 4, 5]\n",
            "[]\n",
            "[6, 1, 7]\n",
            "[]\n",
            "[8, 1, 9, 10, 1, 11]\n",
            "[]\n",
            "학습에 사용할 샘플의 개수: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQWEDppZao3b",
        "outputId": "d6cbc0fe-39b5-4278-a08b-a458345dc1e4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 3], [2, 3, 1], [2, 3, 1, 4], [2, 3, 1, 4, 5], [6, 1], [6, 1, 7], [8, 1], [8, 1, 9], [8, 1, 9, 10], [8, 1, 9, 10, 1], [8, 1, 9, 10, 1, 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 데이터는 아직 레이블로 사용될 단어를 분리하지 않은 훈련 데이터입니다. [2, 3]은 [경마장에, 있는]에 해당되며 [2, 3, 1]은 [경마장에, 있는, 말이]에 해당됩니다. 전체 훈련 데이터에 대해서 맨 우측에 있는 단어에 대해서만 레이블로 분리해야 합니다.\n",
        "\n",
        "우선 전체 샘플에 대해서 길이를 일치시켜 줍니다. 가장 긴 샘플의 길이를 기준으로 합니다. 현재 육안으로 봤을 때, 길이가 가장 긴 샘플은 [8, 1, 9, 10, 1, 11]이고 길이는 6입니다. 이를 코드로는 다음과 같이 구할 수 있습니다."
      ],
      "metadata": {
        "id": "bSGGiroaa3ZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max(len(l) for l in sequences)\n",
        "print('샘플의 최대 길이 : {}'.format(max_len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaD4slmKa0EP",
        "outputId": "0c26c32f-eafd-4c45-bce0-2038b7b4da67"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "샘플의 최대 길이 : 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "전체 훈련 데이터에서 가장 긴 샘플의 길이가 6임을 확인하였다. 전체 샘플의 길이를 6으로 패딩한다."
      ],
      "metadata": {
        "id": "gr3QFmFNbKr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = pad_sequences(sequences, maxlen=max_len,padding='pre')"
      ],
      "metadata": {
        "id": "Rxm-2ppNbDHJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgP8LtRdbTRO",
        "outputId": "40c5f095-41a1-4c3f-d89a-fe41eb5a0944"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  0  0  0  2  3]\n",
            " [ 0  0  0  2  3  1]\n",
            " [ 0  0  2  3  1  4]\n",
            " [ 0  2  3  1  4  5]\n",
            " [ 0  0  0  0  6  1]\n",
            " [ 0  0  0  6  1  7]\n",
            " [ 0  0  0  0  8  1]\n",
            " [ 0  0  0  8  1  9]\n",
            " [ 0  0  8  1  9 10]\n",
            " [ 0  8  1  9 10  1]\n",
            " [ 8  1  9 10  1 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "길이가 6보다 짧은 모든 샘플에 대해서 앞에 0을 채워서 모든 샘플의 길이를 6으로 바꿨습니다. 이제 각 샘플의 마지막 단어를 레이블로 분리합시다. 레이블의 분리는 Numpy를 이용해서 가능합니다. 리스트의 마지막 값을 제외하고 저장한 것은 X, 리스트의 마지막 값만 저장한 것은 y. 이는 레이블에 해당됩니다."
      ],
      "metadata": {
        "id": "d953Y_VybYjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = np.array(sequences)\n",
        "X = sequences[:,:-1]\n",
        "y = sequences[:,-1]"
      ],
      "metadata": {
        "id": "OaGm2qnGbWU-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jkbnzyJbgug",
        "outputId": "f96f8db5-5397-4011-f9ac-2b5a33f797bf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  0  0  0  2]\n",
            " [ 0  0  0  2  3]\n",
            " [ 0  0  2  3  1]\n",
            " [ 0  2  3  1  4]\n",
            " [ 0  0  0  0  6]\n",
            " [ 0  0  0  6  1]\n",
            " [ 0  0  0  0  8]\n",
            " [ 0  0  0  8  1]\n",
            " [ 0  0  8  1  9]\n",
            " [ 0  8  1  9 10]\n",
            " [ 8  1  9 10  1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_McHOuqbiBL",
        "outputId": "6f9ca364-c051-4539-e4ee-9fb7249fc9d3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 3  1  4  5  1  7  1  9 10  1 11]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "레이블이 분리되었다. RNN 모델에 훈련 데이터를 훈련 시키기 전에 레이블에 대해서 원-핫 인코딩을 수행한다."
      ],
      "metadata": {
        "id": "AGbM_yhmbl6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = to_categorical(y,num_classes=vocab_size)"
      ],
      "metadata": {
        "id": "0zQQKn8hbjd7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9B0YYUebt6E",
        "outputId": "b764e94c-a382-4d86-bdac-da3312ca3e54"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 모델 설계하기"
      ],
      "metadata": {
        "id": "8VvunfaIb9R9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN, LSTM"
      ],
      "metadata": {
        "id": "ru8S_Fm7bvL8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 10\n",
        "hidden_units = 128\n",
        "\n",
        "model1 = Sequential()\n",
        "model1.add(Embedding(vocab_size, embedding_dim))\n",
        "model1.add(LSTM(hidden_units))\n",
        "model1.add(Dense(vocab_size, activation='softmax'))\n",
        "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model1.fit(X, y, epochs=200, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIuqUmaMcYTb",
        "outputId": "c5693f00-fd0a-4581-81d2-56220dbfa7de"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1/1 - 2s - loss: 2.4858 - accuracy: 0.0000e+00 - 2s/epoch - 2s/step\n",
            "Epoch 2/200\n",
            "1/1 - 0s - loss: 2.4799 - accuracy: 0.4545 - 11ms/epoch - 11ms/step\n",
            "Epoch 3/200\n",
            "1/1 - 0s - loss: 2.4741 - accuracy: 0.3636 - 9ms/epoch - 9ms/step\n",
            "Epoch 4/200\n",
            "1/1 - 0s - loss: 2.4680 - accuracy: 0.3636 - 9ms/epoch - 9ms/step\n",
            "Epoch 5/200\n",
            "1/1 - 0s - loss: 2.4615 - accuracy: 0.3636 - 9ms/epoch - 9ms/step\n",
            "Epoch 6/200\n",
            "1/1 - 0s - loss: 2.4546 - accuracy: 0.3636 - 10ms/epoch - 10ms/step\n",
            "Epoch 7/200\n",
            "1/1 - 0s - loss: 2.4471 - accuracy: 0.3636 - 9ms/epoch - 9ms/step\n",
            "Epoch 8/200\n",
            "1/1 - 0s - loss: 2.4388 - accuracy: 0.3636 - 10ms/epoch - 10ms/step\n",
            "Epoch 9/200\n",
            "1/1 - 0s - loss: 2.4296 - accuracy: 0.3636 - 13ms/epoch - 13ms/step\n",
            "Epoch 10/200\n",
            "1/1 - 0s - loss: 2.4194 - accuracy: 0.3636 - 11ms/epoch - 11ms/step\n",
            "Epoch 11/200\n",
            "1/1 - 0s - loss: 2.4080 - accuracy: 0.3636 - 10ms/epoch - 10ms/step\n",
            "Epoch 12/200\n",
            "1/1 - 0s - loss: 2.3951 - accuracy: 0.3636 - 11ms/epoch - 11ms/step\n",
            "Epoch 13/200\n",
            "1/1 - 0s - loss: 2.3807 - accuracy: 0.3636 - 11ms/epoch - 11ms/step\n",
            "Epoch 14/200\n",
            "1/1 - 0s - loss: 2.3643 - accuracy: 0.3636 - 12ms/epoch - 12ms/step\n",
            "Epoch 15/200\n",
            "1/1 - 0s - loss: 2.3459 - accuracy: 0.3636 - 9ms/epoch - 9ms/step\n",
            "Epoch 16/200\n",
            "1/1 - 0s - loss: 2.3251 - accuracy: 0.3636 - 15ms/epoch - 15ms/step\n",
            "Epoch 17/200\n",
            "1/1 - 0s - loss: 2.3017 - accuracy: 0.3636 - 10ms/epoch - 10ms/step\n",
            "Epoch 18/200\n",
            "1/1 - 0s - loss: 2.2753 - accuracy: 0.3636 - 9ms/epoch - 9ms/step\n",
            "Epoch 19/200\n",
            "1/1 - 0s - loss: 2.2457 - accuracy: 0.3636 - 9ms/epoch - 9ms/step\n",
            "Epoch 20/200\n",
            "1/1 - 0s - loss: 2.2129 - accuracy: 0.3636 - 9ms/epoch - 9ms/step\n",
            "Epoch 21/200\n",
            "1/1 - 0s - loss: 2.1768 - accuracy: 0.3636 - 14ms/epoch - 14ms/step\n",
            "Epoch 22/200\n",
            "1/1 - 0s - loss: 2.1376 - accuracy: 0.3636 - 9ms/epoch - 9ms/step\n",
            "Epoch 23/200\n",
            "1/1 - 0s - loss: 2.0964 - accuracy: 0.3636 - 9ms/epoch - 9ms/step\n",
            "Epoch 24/200\n",
            "1/1 - 0s - loss: 2.0547 - accuracy: 0.3636 - 11ms/epoch - 11ms/step\n",
            "Epoch 25/200\n",
            "1/1 - 0s - loss: 2.0155 - accuracy: 0.3636 - 9ms/epoch - 9ms/step\n",
            "Epoch 26/200\n",
            "1/1 - 0s - loss: 1.9836 - accuracy: 0.3636 - 9ms/epoch - 9ms/step\n",
            "Epoch 27/200\n",
            "1/1 - 0s - loss: 1.9654 - accuracy: 0.3636 - 14ms/epoch - 14ms/step\n",
            "Epoch 28/200\n",
            "1/1 - 0s - loss: 1.9652 - accuracy: 0.3636 - 11ms/epoch - 11ms/step\n",
            "Epoch 29/200\n",
            "1/1 - 0s - loss: 1.9764 - accuracy: 0.3636 - 11ms/epoch - 11ms/step\n",
            "Epoch 30/200\n",
            "1/1 - 0s - loss: 1.9830 - accuracy: 0.3636 - 10ms/epoch - 10ms/step\n",
            "Epoch 31/200\n",
            "1/1 - 0s - loss: 1.9758 - accuracy: 0.3636 - 10ms/epoch - 10ms/step\n",
            "Epoch 32/200\n",
            "1/1 - 0s - loss: 1.9569 - accuracy: 0.3636 - 11ms/epoch - 11ms/step\n",
            "Epoch 33/200\n",
            "1/1 - 0s - loss: 1.9332 - accuracy: 0.3636 - 11ms/epoch - 11ms/step\n",
            "Epoch 34/200\n",
            "1/1 - 0s - loss: 1.9112 - accuracy: 0.3636 - 11ms/epoch - 11ms/step\n",
            "Epoch 35/200\n",
            "1/1 - 0s - loss: 1.8944 - accuracy: 0.3636 - 11ms/epoch - 11ms/step\n",
            "Epoch 36/200\n",
            "1/1 - 0s - loss: 1.8834 - accuracy: 0.3636 - 11ms/epoch - 11ms/step\n",
            "Epoch 37/200\n",
            "1/1 - 0s - loss: 1.8771 - accuracy: 0.3636 - 11ms/epoch - 11ms/step\n",
            "Epoch 38/200\n",
            "1/1 - 0s - loss: 1.8733 - accuracy: 0.3636 - 11ms/epoch - 11ms/step\n",
            "Epoch 39/200\n",
            "1/1 - 0s - loss: 1.8702 - accuracy: 0.3636 - 11ms/epoch - 11ms/step\n",
            "Epoch 40/200\n",
            "1/1 - 0s - loss: 1.8665 - accuracy: 0.3636 - 11ms/epoch - 11ms/step\n",
            "Epoch 41/200\n",
            "1/1 - 0s - loss: 1.8613 - accuracy: 0.3636 - 11ms/epoch - 11ms/step\n",
            "Epoch 42/200\n",
            "1/1 - 0s - loss: 1.8544 - accuracy: 0.3636 - 13ms/epoch - 13ms/step\n",
            "Epoch 43/200\n",
            "1/1 - 0s - loss: 1.8457 - accuracy: 0.3636 - 12ms/epoch - 12ms/step\n",
            "Epoch 44/200\n",
            "1/1 - 0s - loss: 1.8355 - accuracy: 0.3636 - 13ms/epoch - 13ms/step\n",
            "Epoch 45/200\n",
            "1/1 - 0s - loss: 1.8243 - accuracy: 0.3636 - 12ms/epoch - 12ms/step\n",
            "Epoch 46/200\n",
            "1/1 - 0s - loss: 1.8128 - accuracy: 0.3636 - 12ms/epoch - 12ms/step\n",
            "Epoch 47/200\n",
            "1/1 - 0s - loss: 1.8017 - accuracy: 0.3636 - 12ms/epoch - 12ms/step\n",
            "Epoch 48/200\n",
            "1/1 - 0s - loss: 1.7918 - accuracy: 0.3636 - 10ms/epoch - 10ms/step\n",
            "Epoch 49/200\n",
            "1/1 - 0s - loss: 1.7832 - accuracy: 0.3636 - 11ms/epoch - 11ms/step\n",
            "Epoch 50/200\n",
            "1/1 - 0s - loss: 1.7759 - accuracy: 0.3636 - 11ms/epoch - 11ms/step\n",
            "Epoch 51/200\n",
            "1/1 - 0s - loss: 1.7689 - accuracy: 0.3636 - 13ms/epoch - 13ms/step\n",
            "Epoch 52/200\n",
            "1/1 - 0s - loss: 1.7610 - accuracy: 0.3636 - 10ms/epoch - 10ms/step\n",
            "Epoch 53/200\n",
            "1/1 - 0s - loss: 1.7514 - accuracy: 0.3636 - 10ms/epoch - 10ms/step\n",
            "Epoch 54/200\n",
            "1/1 - 0s - loss: 1.7399 - accuracy: 0.3636 - 14ms/epoch - 14ms/step\n",
            "Epoch 55/200\n",
            "1/1 - 0s - loss: 1.7271 - accuracy: 0.3636 - 9ms/epoch - 9ms/step\n",
            "Epoch 56/200\n",
            "1/1 - 0s - loss: 1.7140 - accuracy: 0.3636 - 11ms/epoch - 11ms/step\n",
            "Epoch 57/200\n",
            "1/1 - 0s - loss: 1.7012 - accuracy: 0.3636 - 9ms/epoch - 9ms/step\n",
            "Epoch 58/200\n",
            "1/1 - 0s - loss: 1.6888 - accuracy: 0.3636 - 9ms/epoch - 9ms/step\n",
            "Epoch 59/200\n",
            "1/1 - 0s - loss: 1.6762 - accuracy: 0.3636 - 14ms/epoch - 14ms/step\n",
            "Epoch 60/200\n",
            "1/1 - 0s - loss: 1.6629 - accuracy: 0.3636 - 12ms/epoch - 12ms/step\n",
            "Epoch 61/200\n",
            "1/1 - 0s - loss: 1.6482 - accuracy: 0.3636 - 11ms/epoch - 11ms/step\n",
            "Epoch 62/200\n",
            "1/1 - 0s - loss: 1.6319 - accuracy: 0.4545 - 10ms/epoch - 10ms/step\n",
            "Epoch 63/200\n",
            "1/1 - 0s - loss: 1.6144 - accuracy: 0.4545 - 9ms/epoch - 9ms/step\n",
            "Epoch 64/200\n",
            "1/1 - 0s - loss: 1.5963 - accuracy: 0.4545 - 11ms/epoch - 11ms/step\n",
            "Epoch 65/200\n",
            "1/1 - 0s - loss: 1.5780 - accuracy: 0.4545 - 11ms/epoch - 11ms/step\n",
            "Epoch 66/200\n",
            "1/1 - 0s - loss: 1.5598 - accuracy: 0.4545 - 12ms/epoch - 12ms/step\n",
            "Epoch 67/200\n",
            "1/1 - 0s - loss: 1.5412 - accuracy: 0.4545 - 13ms/epoch - 13ms/step\n",
            "Epoch 68/200\n",
            "1/1 - 0s - loss: 1.5215 - accuracy: 0.5455 - 11ms/epoch - 11ms/step\n",
            "Epoch 69/200\n",
            "1/1 - 0s - loss: 1.5007 - accuracy: 0.5455 - 9ms/epoch - 9ms/step\n",
            "Epoch 70/200\n",
            "1/1 - 0s - loss: 1.4793 - accuracy: 0.5455 - 9ms/epoch - 9ms/step\n",
            "Epoch 71/200\n",
            "1/1 - 0s - loss: 1.4578 - accuracy: 0.5455 - 9ms/epoch - 9ms/step\n",
            "Epoch 72/200\n",
            "1/1 - 0s - loss: 1.4366 - accuracy: 0.5455 - 11ms/epoch - 11ms/step\n",
            "Epoch 73/200\n",
            "1/1 - 0s - loss: 1.4155 - accuracy: 0.4545 - 12ms/epoch - 12ms/step\n",
            "Epoch 74/200\n",
            "1/1 - 0s - loss: 1.3942 - accuracy: 0.4545 - 10ms/epoch - 10ms/step\n",
            "Epoch 75/200\n",
            "1/1 - 0s - loss: 1.3728 - accuracy: 0.4545 - 10ms/epoch - 10ms/step\n",
            "Epoch 76/200\n",
            "1/1 - 0s - loss: 1.3514 - accuracy: 0.4545 - 11ms/epoch - 11ms/step\n",
            "Epoch 77/200\n",
            "1/1 - 0s - loss: 1.3305 - accuracy: 0.4545 - 10ms/epoch - 10ms/step\n",
            "Epoch 78/200\n",
            "1/1 - 0s - loss: 1.3102 - accuracy: 0.4545 - 9ms/epoch - 9ms/step\n",
            "Epoch 79/200\n",
            "1/1 - 0s - loss: 1.2904 - accuracy: 0.4545 - 11ms/epoch - 11ms/step\n",
            "Epoch 80/200\n",
            "1/1 - 0s - loss: 1.2708 - accuracy: 0.4545 - 9ms/epoch - 9ms/step\n",
            "Epoch 81/200\n",
            "1/1 - 0s - loss: 1.2511 - accuracy: 0.4545 - 9ms/epoch - 9ms/step\n",
            "Epoch 82/200\n",
            "1/1 - 0s - loss: 1.2316 - accuracy: 0.4545 - 9ms/epoch - 9ms/step\n",
            "Epoch 83/200\n",
            "1/1 - 0s - loss: 1.2125 - accuracy: 0.4545 - 9ms/epoch - 9ms/step\n",
            "Epoch 84/200\n",
            "1/1 - 0s - loss: 1.1929 - accuracy: 0.4545 - 13ms/epoch - 13ms/step\n",
            "Epoch 85/200\n",
            "1/1 - 0s - loss: 1.1726 - accuracy: 0.5455 - 11ms/epoch - 11ms/step\n",
            "Epoch 86/200\n",
            "1/1 - 0s - loss: 1.1520 - accuracy: 0.5455 - 11ms/epoch - 11ms/step\n",
            "Epoch 87/200\n",
            "1/1 - 0s - loss: 1.1312 - accuracy: 0.6364 - 11ms/epoch - 11ms/step\n",
            "Epoch 88/200\n",
            "1/1 - 0s - loss: 1.1100 - accuracy: 0.5455 - 12ms/epoch - 12ms/step\n",
            "Epoch 89/200\n",
            "1/1 - 0s - loss: 1.0886 - accuracy: 0.6364 - 13ms/epoch - 13ms/step\n",
            "Epoch 90/200\n",
            "1/1 - 0s - loss: 1.0671 - accuracy: 0.6364 - 12ms/epoch - 12ms/step\n",
            "Epoch 91/200\n",
            "1/1 - 0s - loss: 1.0459 - accuracy: 0.6364 - 11ms/epoch - 11ms/step\n",
            "Epoch 92/200\n",
            "1/1 - 0s - loss: 1.0245 - accuracy: 0.6364 - 12ms/epoch - 12ms/step\n",
            "Epoch 93/200\n",
            "1/1 - 0s - loss: 1.0027 - accuracy: 0.6364 - 13ms/epoch - 13ms/step\n",
            "Epoch 94/200\n",
            "1/1 - 0s - loss: 0.9810 - accuracy: 0.6364 - 17ms/epoch - 17ms/step\n",
            "Epoch 95/200\n",
            "1/1 - 0s - loss: 0.9593 - accuracy: 0.6364 - 12ms/epoch - 12ms/step\n",
            "Epoch 96/200\n",
            "1/1 - 0s - loss: 0.9378 - accuracy: 0.6364 - 11ms/epoch - 11ms/step\n",
            "Epoch 97/200\n",
            "1/1 - 0s - loss: 0.9167 - accuracy: 0.6364 - 10ms/epoch - 10ms/step\n",
            "Epoch 98/200\n",
            "1/1 - 0s - loss: 0.8961 - accuracy: 0.6364 - 10ms/epoch - 10ms/step\n",
            "Epoch 99/200\n",
            "1/1 - 0s - loss: 0.8759 - accuracy: 0.7273 - 11ms/epoch - 11ms/step\n",
            "Epoch 100/200\n",
            "1/1 - 0s - loss: 0.8565 - accuracy: 0.7273 - 12ms/epoch - 12ms/step\n",
            "Epoch 101/200\n",
            "1/1 - 0s - loss: 0.8374 - accuracy: 0.7273 - 14ms/epoch - 14ms/step\n",
            "Epoch 102/200\n",
            "1/1 - 0s - loss: 0.8190 - accuracy: 0.7273 - 11ms/epoch - 11ms/step\n",
            "Epoch 103/200\n",
            "1/1 - 0s - loss: 0.8011 - accuracy: 0.7273 - 10ms/epoch - 10ms/step\n",
            "Epoch 104/200\n",
            "1/1 - 0s - loss: 0.7837 - accuracy: 0.7273 - 11ms/epoch - 11ms/step\n",
            "Epoch 105/200\n",
            "1/1 - 0s - loss: 0.7670 - accuracy: 0.7273 - 11ms/epoch - 11ms/step\n",
            "Epoch 106/200\n",
            "1/1 - 0s - loss: 0.7508 - accuracy: 0.7273 - 10ms/epoch - 10ms/step\n",
            "Epoch 107/200\n",
            "1/1 - 0s - loss: 0.7353 - accuracy: 0.7273 - 10ms/epoch - 10ms/step\n",
            "Epoch 108/200\n",
            "1/1 - 0s - loss: 0.7201 - accuracy: 0.7273 - 10ms/epoch - 10ms/step\n",
            "Epoch 109/200\n",
            "1/1 - 0s - loss: 0.7054 - accuracy: 0.7273 - 11ms/epoch - 11ms/step\n",
            "Epoch 110/200\n",
            "1/1 - 0s - loss: 0.6911 - accuracy: 0.7273 - 11ms/epoch - 11ms/step\n",
            "Epoch 111/200\n",
            "1/1 - 0s - loss: 0.6773 - accuracy: 0.7273 - 11ms/epoch - 11ms/step\n",
            "Epoch 112/200\n",
            "1/1 - 0s - loss: 0.6638 - accuracy: 0.7273 - 10ms/epoch - 10ms/step\n",
            "Epoch 113/200\n",
            "1/1 - 0s - loss: 0.6509 - accuracy: 0.7273 - 10ms/epoch - 10ms/step\n",
            "Epoch 114/200\n",
            "1/1 - 0s - loss: 0.6385 - accuracy: 0.7273 - 11ms/epoch - 11ms/step\n",
            "Epoch 115/200\n",
            "1/1 - 0s - loss: 0.6267 - accuracy: 0.7273 - 11ms/epoch - 11ms/step\n",
            "Epoch 116/200\n",
            "1/1 - 0s - loss: 0.6153 - accuracy: 0.7273 - 11ms/epoch - 11ms/step\n",
            "Epoch 117/200\n",
            "1/1 - 0s - loss: 0.6042 - accuracy: 0.7273 - 13ms/epoch - 13ms/step\n",
            "Epoch 118/200\n",
            "1/1 - 0s - loss: 0.5933 - accuracy: 0.8182 - 12ms/epoch - 12ms/step\n",
            "Epoch 119/200\n",
            "1/1 - 0s - loss: 0.5825 - accuracy: 0.7273 - 10ms/epoch - 10ms/step\n",
            "Epoch 120/200\n",
            "1/1 - 0s - loss: 0.5719 - accuracy: 0.8182 - 11ms/epoch - 11ms/step\n",
            "Epoch 121/200\n",
            "1/1 - 0s - loss: 0.5614 - accuracy: 0.8182 - 12ms/epoch - 12ms/step\n",
            "Epoch 122/200\n",
            "1/1 - 0s - loss: 0.5511 - accuracy: 0.8182 - 12ms/epoch - 12ms/step\n",
            "Epoch 123/200\n",
            "1/1 - 0s - loss: 0.5412 - accuracy: 0.8182 - 19ms/epoch - 19ms/step\n",
            "Epoch 124/200\n",
            "1/1 - 0s - loss: 0.5315 - accuracy: 0.8182 - 11ms/epoch - 11ms/step\n",
            "Epoch 125/200\n",
            "1/1 - 0s - loss: 0.5220 - accuracy: 0.8182 - 14ms/epoch - 14ms/step\n",
            "Epoch 126/200\n",
            "1/1 - 0s - loss: 0.5125 - accuracy: 0.8182 - 12ms/epoch - 12ms/step\n",
            "Epoch 127/200\n",
            "1/1 - 0s - loss: 0.5031 - accuracy: 0.8182 - 10ms/epoch - 10ms/step\n",
            "Epoch 128/200\n",
            "1/1 - 0s - loss: 0.4937 - accuracy: 0.8182 - 13ms/epoch - 13ms/step\n",
            "Epoch 129/200\n",
            "1/1 - 0s - loss: 0.4844 - accuracy: 0.8182 - 13ms/epoch - 13ms/step\n",
            "Epoch 130/200\n",
            "1/1 - 0s - loss: 0.4757 - accuracy: 0.8182 - 12ms/epoch - 12ms/step\n",
            "Epoch 131/200\n",
            "1/1 - 0s - loss: 0.4690 - accuracy: 0.9091 - 11ms/epoch - 11ms/step\n",
            "Epoch 132/200\n",
            "1/1 - 0s - loss: 0.4610 - accuracy: 0.8182 - 12ms/epoch - 12ms/step\n",
            "Epoch 133/200\n",
            "1/1 - 0s - loss: 0.4488 - accuracy: 0.9091 - 13ms/epoch - 13ms/step\n",
            "Epoch 134/200\n",
            "1/1 - 0s - loss: 0.4398 - accuracy: 0.9091 - 11ms/epoch - 11ms/step\n",
            "Epoch 135/200\n",
            "1/1 - 0s - loss: 0.4327 - accuracy: 0.8182 - 11ms/epoch - 11ms/step\n",
            "Epoch 136/200\n",
            "1/1 - 0s - loss: 0.4224 - accuracy: 0.9091 - 12ms/epoch - 12ms/step\n",
            "Epoch 137/200\n",
            "1/1 - 0s - loss: 0.4149 - accuracy: 0.9091 - 12ms/epoch - 12ms/step\n",
            "Epoch 138/200\n",
            "1/1 - 0s - loss: 0.4069 - accuracy: 0.9091 - 10ms/epoch - 10ms/step\n",
            "Epoch 139/200\n",
            "1/1 - 0s - loss: 0.3985 - accuracy: 0.9091 - 13ms/epoch - 13ms/step\n",
            "Epoch 140/200\n",
            "1/1 - 0s - loss: 0.3909 - accuracy: 0.9091 - 11ms/epoch - 11ms/step\n",
            "Epoch 141/200\n",
            "1/1 - 0s - loss: 0.3828 - accuracy: 0.9091 - 11ms/epoch - 11ms/step\n",
            "Epoch 142/200\n",
            "1/1 - 0s - loss: 0.3752 - accuracy: 0.9091 - 11ms/epoch - 11ms/step\n",
            "Epoch 143/200\n",
            "1/1 - 0s - loss: 0.3677 - accuracy: 0.9091 - 11ms/epoch - 11ms/step\n",
            "Epoch 144/200\n",
            "1/1 - 0s - loss: 0.3605 - accuracy: 0.9091 - 11ms/epoch - 11ms/step\n",
            "Epoch 145/200\n",
            "1/1 - 0s - loss: 0.3531 - accuracy: 0.9091 - 18ms/epoch - 18ms/step\n",
            "Epoch 146/200\n",
            "1/1 - 0s - loss: 0.3465 - accuracy: 0.9091 - 11ms/epoch - 11ms/step\n",
            "Epoch 147/200\n",
            "1/1 - 0s - loss: 0.3391 - accuracy: 0.9091 - 11ms/epoch - 11ms/step\n",
            "Epoch 148/200\n",
            "1/1 - 0s - loss: 0.3329 - accuracy: 0.9091 - 12ms/epoch - 12ms/step\n",
            "Epoch 149/200\n",
            "1/1 - 0s - loss: 0.3258 - accuracy: 0.9091 - 11ms/epoch - 11ms/step\n",
            "Epoch 150/200\n",
            "1/1 - 0s - loss: 0.3193 - accuracy: 0.9091 - 10ms/epoch - 10ms/step\n",
            "Epoch 151/200\n",
            "1/1 - 0s - loss: 0.3130 - accuracy: 0.9091 - 11ms/epoch - 11ms/step\n",
            "Epoch 152/200\n",
            "1/1 - 0s - loss: 0.3061 - accuracy: 0.9091 - 11ms/epoch - 11ms/step\n",
            "Epoch 153/200\n",
            "1/1 - 0s - loss: 0.3003 - accuracy: 0.9091 - 13ms/epoch - 13ms/step\n",
            "Epoch 154/200\n",
            "1/1 - 0s - loss: 0.2937 - accuracy: 0.9091 - 10ms/epoch - 10ms/step\n",
            "Epoch 155/200\n",
            "1/1 - 0s - loss: 0.2879 - accuracy: 0.9091 - 14ms/epoch - 14ms/step\n",
            "Epoch 156/200\n",
            "1/1 - 0s - loss: 0.2819 - accuracy: 0.9091 - 10ms/epoch - 10ms/step\n",
            "Epoch 157/200\n",
            "1/1 - 0s - loss: 0.2760 - accuracy: 0.9091 - 10ms/epoch - 10ms/step\n",
            "Epoch 158/200\n",
            "1/1 - 0s - loss: 0.2706 - accuracy: 0.9091 - 11ms/epoch - 11ms/step\n",
            "Epoch 159/200\n",
            "1/1 - 0s - loss: 0.2649 - accuracy: 0.9091 - 10ms/epoch - 10ms/step\n",
            "Epoch 160/200\n",
            "1/1 - 0s - loss: 0.2594 - accuracy: 0.9091 - 13ms/epoch - 13ms/step\n",
            "Epoch 161/200\n",
            "1/1 - 0s - loss: 0.2541 - accuracy: 0.9091 - 11ms/epoch - 11ms/step\n",
            "Epoch 162/200\n",
            "1/1 - 0s - loss: 0.2487 - accuracy: 0.9091 - 13ms/epoch - 13ms/step\n",
            "Epoch 163/200\n",
            "1/1 - 0s - loss: 0.2435 - accuracy: 0.9091 - 8ms/epoch - 8ms/step\n",
            "Epoch 164/200\n",
            "1/1 - 0s - loss: 0.2384 - accuracy: 0.9091 - 11ms/epoch - 11ms/step\n",
            "Epoch 165/200\n",
            "1/1 - 0s - loss: 0.2331 - accuracy: 0.9091 - 11ms/epoch - 11ms/step\n",
            "Epoch 166/200\n",
            "1/1 - 0s - loss: 0.2282 - accuracy: 0.9091 - 10ms/epoch - 10ms/step\n",
            "Epoch 167/200\n",
            "1/1 - 0s - loss: 0.2231 - accuracy: 0.9091 - 11ms/epoch - 11ms/step\n",
            "Epoch 168/200\n",
            "1/1 - 0s - loss: 0.2181 - accuracy: 0.9091 - 10ms/epoch - 10ms/step\n",
            "Epoch 169/200\n",
            "1/1 - 0s - loss: 0.2132 - accuracy: 0.9091 - 10ms/epoch - 10ms/step\n",
            "Epoch 170/200\n",
            "1/1 - 0s - loss: 0.2083 - accuracy: 0.9091 - 11ms/epoch - 11ms/step\n",
            "Epoch 171/200\n",
            "1/1 - 0s - loss: 0.2033 - accuracy: 0.9091 - 11ms/epoch - 11ms/step\n",
            "Epoch 172/200\n",
            "1/1 - 0s - loss: 0.1983 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 173/200\n",
            "1/1 - 0s - loss: 0.1932 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 174/200\n",
            "1/1 - 0s - loss: 0.1881 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 175/200\n",
            "1/1 - 0s - loss: 0.1833 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 176/200\n",
            "1/1 - 0s - loss: 0.1791 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 177/200\n",
            "1/1 - 0s - loss: 0.1751 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 178/200\n",
            "1/1 - 0s - loss: 0.1710 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 179/200\n",
            "1/1 - 0s - loss: 0.1667 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 180/200\n",
            "1/1 - 0s - loss: 0.1624 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 181/200\n",
            "1/1 - 0s - loss: 0.1581 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 182/200\n",
            "1/1 - 0s - loss: 0.1540 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 183/200\n",
            "1/1 - 0s - loss: 0.1499 - accuracy: 1.0000 - 12ms/epoch - 12ms/step\n",
            "Epoch 184/200\n",
            "1/1 - 0s - loss: 0.1461 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 185/200\n",
            "1/1 - 0s - loss: 0.1424 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 186/200\n",
            "1/1 - 0s - loss: 0.1386 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 187/200\n",
            "1/1 - 0s - loss: 0.1349 - accuracy: 1.0000 - 12ms/epoch - 12ms/step\n",
            "Epoch 188/200\n",
            "1/1 - 0s - loss: 0.1313 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 189/200\n",
            "1/1 - 0s - loss: 0.1277 - accuracy: 1.0000 - 12ms/epoch - 12ms/step\n",
            "Epoch 190/200\n",
            "1/1 - 0s - loss: 0.1242 - accuracy: 1.0000 - 12ms/epoch - 12ms/step\n",
            "Epoch 191/200\n",
            "1/1 - 0s - loss: 0.1209 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 192/200\n",
            "1/1 - 0s - loss: 0.1177 - accuracy: 1.0000 - 12ms/epoch - 12ms/step\n",
            "Epoch 193/200\n",
            "1/1 - 0s - loss: 0.1145 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 194/200\n",
            "1/1 - 0s - loss: 0.1114 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 195/200\n",
            "1/1 - 0s - loss: 0.1083 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 196/200\n",
            "1/1 - 0s - loss: 0.1053 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 197/200\n",
            "1/1 - 0s - loss: 0.1024 - accuracy: 1.0000 - 18ms/epoch - 18ms/step\n",
            "Epoch 198/200\n",
            "1/1 - 0s - loss: 0.0997 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 199/200\n",
            "1/1 - 0s - loss: 0.0969 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 200/200\n",
            "1/1 - 0s - loss: 0.0943 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcbde1d2b50>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 10\n",
        "hidden_units = 32\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(SimpleRNN(hidden_units))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=200, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8cMIzdNcCeR",
        "outputId": "7aa9dfce-d727-4c7f-e497-f4ce6d0b3844"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1/1 - 1s - loss: 2.4675 - accuracy: 0.1818 - 1s/epoch - 1s/step\n",
            "Epoch 2/200\n",
            "1/1 - 0s - loss: 2.4555 - accuracy: 0.3636 - 10ms/epoch - 10ms/step\n",
            "Epoch 3/200\n",
            "1/1 - 0s - loss: 2.4437 - accuracy: 0.4545 - 8ms/epoch - 8ms/step\n",
            "Epoch 4/200\n",
            "1/1 - 0s - loss: 2.4321 - accuracy: 0.5455 - 9ms/epoch - 9ms/step\n",
            "Epoch 5/200\n",
            "1/1 - 0s - loss: 2.4205 - accuracy: 0.5455 - 9ms/epoch - 9ms/step\n",
            "Epoch 6/200\n",
            "1/1 - 0s - loss: 2.4089 - accuracy: 0.6364 - 9ms/epoch - 9ms/step\n",
            "Epoch 7/200\n",
            "1/1 - 0s - loss: 2.3972 - accuracy: 0.5455 - 7ms/epoch - 7ms/step\n",
            "Epoch 8/200\n",
            "1/1 - 0s - loss: 2.3853 - accuracy: 0.5455 - 6ms/epoch - 6ms/step\n",
            "Epoch 9/200\n",
            "1/1 - 0s - loss: 2.3731 - accuracy: 0.5455 - 8ms/epoch - 8ms/step\n",
            "Epoch 10/200\n",
            "1/1 - 0s - loss: 2.3606 - accuracy: 0.5455 - 7ms/epoch - 7ms/step\n",
            "Epoch 11/200\n",
            "1/1 - 0s - loss: 2.3476 - accuracy: 0.5455 - 8ms/epoch - 8ms/step\n",
            "Epoch 12/200\n",
            "1/1 - 0s - loss: 2.3342 - accuracy: 0.5455 - 8ms/epoch - 8ms/step\n",
            "Epoch 13/200\n",
            "1/1 - 0s - loss: 2.3201 - accuracy: 0.5455 - 8ms/epoch - 8ms/step\n",
            "Epoch 14/200\n",
            "1/1 - 0s - loss: 2.3054 - accuracy: 0.5455 - 11ms/epoch - 11ms/step\n",
            "Epoch 15/200\n",
            "1/1 - 0s - loss: 2.2901 - accuracy: 0.5455 - 8ms/epoch - 8ms/step\n",
            "Epoch 16/200\n",
            "1/1 - 0s - loss: 2.2739 - accuracy: 0.5455 - 7ms/epoch - 7ms/step\n",
            "Epoch 17/200\n",
            "1/1 - 0s - loss: 2.2570 - accuracy: 0.5455 - 7ms/epoch - 7ms/step\n",
            "Epoch 18/200\n",
            "1/1 - 0s - loss: 2.2392 - accuracy: 0.5455 - 6ms/epoch - 6ms/step\n",
            "Epoch 19/200\n",
            "1/1 - 0s - loss: 2.2206 - accuracy: 0.6364 - 7ms/epoch - 7ms/step\n",
            "Epoch 20/200\n",
            "1/1 - 0s - loss: 2.2010 - accuracy: 0.6364 - 5ms/epoch - 5ms/step\n",
            "Epoch 21/200\n",
            "1/1 - 0s - loss: 2.1804 - accuracy: 0.6364 - 10ms/epoch - 10ms/step\n",
            "Epoch 22/200\n",
            "1/1 - 0s - loss: 2.1589 - accuracy: 0.5455 - 6ms/epoch - 6ms/step\n",
            "Epoch 23/200\n",
            "1/1 - 0s - loss: 2.1365 - accuracy: 0.5455 - 7ms/epoch - 7ms/step\n",
            "Epoch 24/200\n",
            "1/1 - 0s - loss: 2.1131 - accuracy: 0.5455 - 8ms/epoch - 8ms/step\n",
            "Epoch 25/200\n",
            "1/1 - 0s - loss: 2.0887 - accuracy: 0.5455 - 6ms/epoch - 6ms/step\n",
            "Epoch 26/200\n",
            "1/1 - 0s - loss: 2.0636 - accuracy: 0.5455 - 9ms/epoch - 9ms/step\n",
            "Epoch 27/200\n",
            "1/1 - 0s - loss: 2.0376 - accuracy: 0.4545 - 7ms/epoch - 7ms/step\n",
            "Epoch 28/200\n",
            "1/1 - 0s - loss: 2.0108 - accuracy: 0.4545 - 7ms/epoch - 7ms/step\n",
            "Epoch 29/200\n",
            "1/1 - 0s - loss: 1.9835 - accuracy: 0.4545 - 12ms/epoch - 12ms/step\n",
            "Epoch 30/200\n",
            "1/1 - 0s - loss: 1.9558 - accuracy: 0.3636 - 8ms/epoch - 8ms/step\n",
            "Epoch 31/200\n",
            "1/1 - 0s - loss: 1.9277 - accuracy: 0.3636 - 7ms/epoch - 7ms/step\n",
            "Epoch 32/200\n",
            "1/1 - 0s - loss: 1.8995 - accuracy: 0.3636 - 8ms/epoch - 8ms/step\n",
            "Epoch 33/200\n",
            "1/1 - 0s - loss: 1.8712 - accuracy: 0.3636 - 6ms/epoch - 6ms/step\n",
            "Epoch 34/200\n",
            "1/1 - 0s - loss: 1.8432 - accuracy: 0.3636 - 7ms/epoch - 7ms/step\n",
            "Epoch 35/200\n",
            "1/1 - 0s - loss: 1.8156 - accuracy: 0.3636 - 7ms/epoch - 7ms/step\n",
            "Epoch 36/200\n",
            "1/1 - 0s - loss: 1.7885 - accuracy: 0.3636 - 6ms/epoch - 6ms/step\n",
            "Epoch 37/200\n",
            "1/1 - 0s - loss: 1.7620 - accuracy: 0.3636 - 9ms/epoch - 9ms/step\n",
            "Epoch 38/200\n",
            "1/1 - 0s - loss: 1.7362 - accuracy: 0.3636 - 5ms/epoch - 5ms/step\n",
            "Epoch 39/200\n",
            "1/1 - 0s - loss: 1.7113 - accuracy: 0.3636 - 7ms/epoch - 7ms/step\n",
            "Epoch 40/200\n",
            "1/1 - 0s - loss: 1.6872 - accuracy: 0.3636 - 7ms/epoch - 7ms/step\n",
            "Epoch 41/200\n",
            "1/1 - 0s - loss: 1.6639 - accuracy: 0.3636 - 7ms/epoch - 7ms/step\n",
            "Epoch 42/200\n",
            "1/1 - 0s - loss: 1.6414 - accuracy: 0.3636 - 5ms/epoch - 5ms/step\n",
            "Epoch 43/200\n",
            "1/1 - 0s - loss: 1.6196 - accuracy: 0.3636 - 7ms/epoch - 7ms/step\n",
            "Epoch 44/200\n",
            "1/1 - 0s - loss: 1.5982 - accuracy: 0.3636 - 6ms/epoch - 6ms/step\n",
            "Epoch 45/200\n",
            "1/1 - 0s - loss: 1.5773 - accuracy: 0.4545 - 6ms/epoch - 6ms/step\n",
            "Epoch 46/200\n",
            "1/1 - 0s - loss: 1.5565 - accuracy: 0.4545 - 5ms/epoch - 5ms/step\n",
            "Epoch 47/200\n",
            "1/1 - 0s - loss: 1.5359 - accuracy: 0.4545 - 7ms/epoch - 7ms/step\n",
            "Epoch 48/200\n",
            "1/1 - 0s - loss: 1.5152 - accuracy: 0.4545 - 6ms/epoch - 6ms/step\n",
            "Epoch 49/200\n",
            "1/1 - 0s - loss: 1.4944 - accuracy: 0.4545 - 7ms/epoch - 7ms/step\n",
            "Epoch 50/200\n",
            "1/1 - 0s - loss: 1.4733 - accuracy: 0.4545 - 8ms/epoch - 8ms/step\n",
            "Epoch 51/200\n",
            "1/1 - 0s - loss: 1.4521 - accuracy: 0.4545 - 9ms/epoch - 9ms/step\n",
            "Epoch 52/200\n",
            "1/1 - 0s - loss: 1.4307 - accuracy: 0.4545 - 7ms/epoch - 7ms/step\n",
            "Epoch 53/200\n",
            "1/1 - 0s - loss: 1.4091 - accuracy: 0.4545 - 8ms/epoch - 8ms/step\n",
            "Epoch 54/200\n",
            "1/1 - 0s - loss: 1.3874 - accuracy: 0.4545 - 8ms/epoch - 8ms/step\n",
            "Epoch 55/200\n",
            "1/1 - 0s - loss: 1.3658 - accuracy: 0.4545 - 7ms/epoch - 7ms/step\n",
            "Epoch 56/200\n",
            "1/1 - 0s - loss: 1.3442 - accuracy: 0.4545 - 6ms/epoch - 6ms/step\n",
            "Epoch 57/200\n",
            "1/1 - 0s - loss: 1.3228 - accuracy: 0.4545 - 8ms/epoch - 8ms/step\n",
            "Epoch 58/200\n",
            "1/1 - 0s - loss: 1.3017 - accuracy: 0.5455 - 9ms/epoch - 9ms/step\n",
            "Epoch 59/200\n",
            "1/1 - 0s - loss: 1.2809 - accuracy: 0.5455 - 7ms/epoch - 7ms/step\n",
            "Epoch 60/200\n",
            "1/1 - 0s - loss: 1.2604 - accuracy: 0.6364 - 8ms/epoch - 8ms/step\n",
            "Epoch 61/200\n",
            "1/1 - 0s - loss: 1.2404 - accuracy: 0.6364 - 7ms/epoch - 7ms/step\n",
            "Epoch 62/200\n",
            "1/1 - 0s - loss: 1.2207 - accuracy: 0.6364 - 9ms/epoch - 9ms/step\n",
            "Epoch 63/200\n",
            "1/1 - 0s - loss: 1.2015 - accuracy: 0.6364 - 8ms/epoch - 8ms/step\n",
            "Epoch 64/200\n",
            "1/1 - 0s - loss: 1.1827 - accuracy: 0.6364 - 10ms/epoch - 10ms/step\n",
            "Epoch 65/200\n",
            "1/1 - 0s - loss: 1.1643 - accuracy: 0.6364 - 8ms/epoch - 8ms/step\n",
            "Epoch 66/200\n",
            "1/1 - 0s - loss: 1.1462 - accuracy: 0.6364 - 6ms/epoch - 6ms/step\n",
            "Epoch 67/200\n",
            "1/1 - 0s - loss: 1.1285 - accuracy: 0.6364 - 7ms/epoch - 7ms/step\n",
            "Epoch 68/200\n",
            "1/1 - 0s - loss: 1.1110 - accuracy: 0.7273 - 11ms/epoch - 11ms/step\n",
            "Epoch 69/200\n",
            "1/1 - 0s - loss: 1.0938 - accuracy: 0.7273 - 9ms/epoch - 9ms/step\n",
            "Epoch 70/200\n",
            "1/1 - 0s - loss: 1.0768 - accuracy: 0.7273 - 8ms/epoch - 8ms/step\n",
            "Epoch 71/200\n",
            "1/1 - 0s - loss: 1.0600 - accuracy: 0.7273 - 8ms/epoch - 8ms/step\n",
            "Epoch 72/200\n",
            "1/1 - 0s - loss: 1.0433 - accuracy: 0.7273 - 10ms/epoch - 10ms/step\n",
            "Epoch 73/200\n",
            "1/1 - 0s - loss: 1.0269 - accuracy: 0.7273 - 10ms/epoch - 10ms/step\n",
            "Epoch 74/200\n",
            "1/1 - 0s - loss: 1.0106 - accuracy: 0.7273 - 9ms/epoch - 9ms/step\n",
            "Epoch 75/200\n",
            "1/1 - 0s - loss: 0.9945 - accuracy: 0.7273 - 6ms/epoch - 6ms/step\n",
            "Epoch 76/200\n",
            "1/1 - 0s - loss: 0.9785 - accuracy: 0.7273 - 8ms/epoch - 8ms/step\n",
            "Epoch 77/200\n",
            "1/1 - 0s - loss: 0.9628 - accuracy: 0.7273 - 8ms/epoch - 8ms/step\n",
            "Epoch 78/200\n",
            "1/1 - 0s - loss: 0.9472 - accuracy: 0.7273 - 7ms/epoch - 7ms/step\n",
            "Epoch 79/200\n",
            "1/1 - 0s - loss: 0.9319 - accuracy: 0.7273 - 6ms/epoch - 6ms/step\n",
            "Epoch 80/200\n",
            "1/1 - 0s - loss: 0.9167 - accuracy: 0.7273 - 8ms/epoch - 8ms/step\n",
            "Epoch 81/200\n",
            "1/1 - 0s - loss: 0.9017 - accuracy: 0.7273 - 8ms/epoch - 8ms/step\n",
            "Epoch 82/200\n",
            "1/1 - 0s - loss: 0.8870 - accuracy: 0.7273 - 6ms/epoch - 6ms/step\n",
            "Epoch 83/200\n",
            "1/1 - 0s - loss: 0.8724 - accuracy: 0.7273 - 8ms/epoch - 8ms/step\n",
            "Epoch 84/200\n",
            "1/1 - 0s - loss: 0.8579 - accuracy: 0.7273 - 9ms/epoch - 9ms/step\n",
            "Epoch 85/200\n",
            "1/1 - 0s - loss: 0.8437 - accuracy: 0.7273 - 9ms/epoch - 9ms/step\n",
            "Epoch 86/200\n",
            "1/1 - 0s - loss: 0.8296 - accuracy: 0.7273 - 7ms/epoch - 7ms/step\n",
            "Epoch 87/200\n",
            "1/1 - 0s - loss: 0.8157 - accuracy: 0.7273 - 7ms/epoch - 7ms/step\n",
            "Epoch 88/200\n",
            "1/1 - 0s - loss: 0.8019 - accuracy: 0.7273 - 11ms/epoch - 11ms/step\n",
            "Epoch 89/200\n",
            "1/1 - 0s - loss: 0.7883 - accuracy: 0.7273 - 9ms/epoch - 9ms/step\n",
            "Epoch 90/200\n",
            "1/1 - 0s - loss: 0.7748 - accuracy: 0.7273 - 7ms/epoch - 7ms/step\n",
            "Epoch 91/200\n",
            "1/1 - 0s - loss: 0.7615 - accuracy: 0.7273 - 7ms/epoch - 7ms/step\n",
            "Epoch 92/200\n",
            "1/1 - 0s - loss: 0.7483 - accuracy: 0.7273 - 9ms/epoch - 9ms/step\n",
            "Epoch 93/200\n",
            "1/1 - 0s - loss: 0.7353 - accuracy: 0.7273 - 7ms/epoch - 7ms/step\n",
            "Epoch 94/200\n",
            "1/1 - 0s - loss: 0.7224 - accuracy: 0.7273 - 7ms/epoch - 7ms/step\n",
            "Epoch 95/200\n",
            "1/1 - 0s - loss: 0.7097 - accuracy: 0.7273 - 7ms/epoch - 7ms/step\n",
            "Epoch 96/200\n",
            "1/1 - 0s - loss: 0.6971 - accuracy: 0.7273 - 8ms/epoch - 8ms/step\n",
            "Epoch 97/200\n",
            "1/1 - 0s - loss: 0.6847 - accuracy: 0.7273 - 10ms/epoch - 10ms/step\n",
            "Epoch 98/200\n",
            "1/1 - 0s - loss: 0.6725 - accuracy: 0.8182 - 9ms/epoch - 9ms/step\n",
            "Epoch 99/200\n",
            "1/1 - 0s - loss: 0.6604 - accuracy: 0.8182 - 6ms/epoch - 6ms/step\n",
            "Epoch 100/200\n",
            "1/1 - 0s - loss: 0.6485 - accuracy: 0.8182 - 7ms/epoch - 7ms/step\n",
            "Epoch 101/200\n",
            "1/1 - 0s - loss: 0.6368 - accuracy: 0.8182 - 7ms/epoch - 7ms/step\n",
            "Epoch 102/200\n",
            "1/1 - 0s - loss: 0.6252 - accuracy: 0.8182 - 7ms/epoch - 7ms/step\n",
            "Epoch 103/200\n",
            "1/1 - 0s - loss: 0.6138 - accuracy: 0.8182 - 7ms/epoch - 7ms/step\n",
            "Epoch 104/200\n",
            "1/1 - 0s - loss: 0.6026 - accuracy: 0.8182 - 7ms/epoch - 7ms/step\n",
            "Epoch 105/200\n",
            "1/1 - 0s - loss: 0.5916 - accuracy: 0.8182 - 8ms/epoch - 8ms/step\n",
            "Epoch 106/200\n",
            "1/1 - 0s - loss: 0.5807 - accuracy: 0.8182 - 7ms/epoch - 7ms/step\n",
            "Epoch 107/200\n",
            "1/1 - 0s - loss: 0.5701 - accuracy: 0.8182 - 7ms/epoch - 7ms/step\n",
            "Epoch 108/200\n",
            "1/1 - 0s - loss: 0.5595 - accuracy: 0.8182 - 7ms/epoch - 7ms/step\n",
            "Epoch 109/200\n",
            "1/1 - 0s - loss: 0.5492 - accuracy: 0.8182 - 7ms/epoch - 7ms/step\n",
            "Epoch 110/200\n",
            "1/1 - 0s - loss: 0.5390 - accuracy: 0.9091 - 7ms/epoch - 7ms/step\n",
            "Epoch 111/200\n",
            "1/1 - 0s - loss: 0.5290 - accuracy: 0.9091 - 7ms/epoch - 7ms/step\n",
            "Epoch 112/200\n",
            "1/1 - 0s - loss: 0.5191 - accuracy: 0.9091 - 7ms/epoch - 7ms/step\n",
            "Epoch 113/200\n",
            "1/1 - 0s - loss: 0.5094 - accuracy: 0.9091 - 7ms/epoch - 7ms/step\n",
            "Epoch 114/200\n",
            "1/1 - 0s - loss: 0.4998 - accuracy: 0.9091 - 7ms/epoch - 7ms/step\n",
            "Epoch 115/200\n",
            "1/1 - 0s - loss: 0.4904 - accuracy: 0.9091 - 6ms/epoch - 6ms/step\n",
            "Epoch 116/200\n",
            "1/1 - 0s - loss: 0.4812 - accuracy: 0.9091 - 7ms/epoch - 7ms/step\n",
            "Epoch 117/200\n",
            "1/1 - 0s - loss: 0.4720 - accuracy: 0.9091 - 7ms/epoch - 7ms/step\n",
            "Epoch 118/200\n",
            "1/1 - 0s - loss: 0.4631 - accuracy: 0.9091 - 7ms/epoch - 7ms/step\n",
            "Epoch 119/200\n",
            "1/1 - 0s - loss: 0.4542 - accuracy: 0.9091 - 7ms/epoch - 7ms/step\n",
            "Epoch 120/200\n",
            "1/1 - 0s - loss: 0.4455 - accuracy: 0.9091 - 8ms/epoch - 8ms/step\n",
            "Epoch 121/200\n",
            "1/1 - 0s - loss: 0.4370 - accuracy: 0.9091 - 7ms/epoch - 7ms/step\n",
            "Epoch 122/200\n",
            "1/1 - 0s - loss: 0.4285 - accuracy: 0.9091 - 7ms/epoch - 7ms/step\n",
            "Epoch 123/200\n",
            "1/1 - 0s - loss: 0.4203 - accuracy: 0.9091 - 9ms/epoch - 9ms/step\n",
            "Epoch 124/200\n",
            "1/1 - 0s - loss: 0.4121 - accuracy: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 125/200\n",
            "1/1 - 0s - loss: 0.4041 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 126/200\n",
            "1/1 - 0s - loss: 0.3962 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 127/200\n",
            "1/1 - 0s - loss: 0.3884 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 128/200\n",
            "1/1 - 0s - loss: 0.3808 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 129/200\n",
            "1/1 - 0s - loss: 0.3733 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 130/200\n",
            "1/1 - 0s - loss: 0.3659 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 131/200\n",
            "1/1 - 0s - loss: 0.3587 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 132/200\n",
            "1/1 - 0s - loss: 0.3516 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 133/200\n",
            "1/1 - 0s - loss: 0.3446 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 134/200\n",
            "1/1 - 0s - loss: 0.3377 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 135/200\n",
            "1/1 - 0s - loss: 0.3309 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 136/200\n",
            "1/1 - 0s - loss: 0.3243 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 137/200\n",
            "1/1 - 0s - loss: 0.3178 - accuracy: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 138/200\n",
            "1/1 - 0s - loss: 0.3114 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 139/200\n",
            "1/1 - 0s - loss: 0.3052 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 140/200\n",
            "1/1 - 0s - loss: 0.2990 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 141/200\n",
            "1/1 - 0s - loss: 0.2930 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 142/200\n",
            "1/1 - 0s - loss: 0.2871 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 143/200\n",
            "1/1 - 0s - loss: 0.2813 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 144/200\n",
            "1/1 - 0s - loss: 0.2756 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 145/200\n",
            "1/1 - 0s - loss: 0.2700 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 146/200\n",
            "1/1 - 0s - loss: 0.2645 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 147/200\n",
            "1/1 - 0s - loss: 0.2592 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 148/200\n",
            "1/1 - 0s - loss: 0.2540 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 149/200\n",
            "1/1 - 0s - loss: 0.2488 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 150/200\n",
            "1/1 - 0s - loss: 0.2438 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 151/200\n",
            "1/1 - 0s - loss: 0.2389 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 152/200\n",
            "1/1 - 0s - loss: 0.2341 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 153/200\n",
            "1/1 - 0s - loss: 0.2294 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 154/200\n",
            "1/1 - 0s - loss: 0.2248 - accuracy: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 155/200\n",
            "1/1 - 0s - loss: 0.2203 - accuracy: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 156/200\n",
            "1/1 - 0s - loss: 0.2158 - accuracy: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 157/200\n",
            "1/1 - 0s - loss: 0.2115 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 158/200\n",
            "1/1 - 0s - loss: 0.2073 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 159/200\n",
            "1/1 - 0s - loss: 0.2032 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 160/200\n",
            "1/1 - 0s - loss: 0.1992 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 161/200\n",
            "1/1 - 0s - loss: 0.1952 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 162/200\n",
            "1/1 - 0s - loss: 0.1914 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 163/200\n",
            "1/1 - 0s - loss: 0.1876 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 164/200\n",
            "1/1 - 0s - loss: 0.1839 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 165/200\n",
            "1/1 - 0s - loss: 0.1803 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 166/200\n",
            "1/1 - 0s - loss: 0.1768 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 167/200\n",
            "1/1 - 0s - loss: 0.1734 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 168/200\n",
            "1/1 - 0s - loss: 0.1700 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 169/200\n",
            "1/1 - 0s - loss: 0.1667 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 170/200\n",
            "1/1 - 0s - loss: 0.1635 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 171/200\n",
            "1/1 - 0s - loss: 0.1604 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 172/200\n",
            "1/1 - 0s - loss: 0.1573 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 173/200\n",
            "1/1 - 0s - loss: 0.1544 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 174/200\n",
            "1/1 - 0s - loss: 0.1514 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 175/200\n",
            "1/1 - 0s - loss: 0.1486 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 176/200\n",
            "1/1 - 0s - loss: 0.1458 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 177/200\n",
            "1/1 - 0s - loss: 0.1431 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 178/200\n",
            "1/1 - 0s - loss: 0.1404 - accuracy: 1.0000 - 12ms/epoch - 12ms/step\n",
            "Epoch 179/200\n",
            "1/1 - 0s - loss: 0.1378 - accuracy: 1.0000 - 12ms/epoch - 12ms/step\n",
            "Epoch 180/200\n",
            "1/1 - 0s - loss: 0.1353 - accuracy: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 181/200\n",
            "1/1 - 0s - loss: 0.1328 - accuracy: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 182/200\n",
            "1/1 - 0s - loss: 0.1304 - accuracy: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 183/200\n",
            "1/1 - 0s - loss: 0.1281 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 184/200\n",
            "1/1 - 0s - loss: 0.1258 - accuracy: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 185/200\n",
            "1/1 - 0s - loss: 0.1235 - accuracy: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 186/200\n",
            "1/1 - 0s - loss: 0.1213 - accuracy: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 187/200\n",
            "1/1 - 0s - loss: 0.1191 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 188/200\n",
            "1/1 - 0s - loss: 0.1170 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 189/200\n",
            "1/1 - 0s - loss: 0.1150 - accuracy: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 190/200\n",
            "1/1 - 0s - loss: 0.1130 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 191/200\n",
            "1/1 - 0s - loss: 0.1110 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 192/200\n",
            "1/1 - 0s - loss: 0.1091 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 193/200\n",
            "1/1 - 0s - loss: 0.1072 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 194/200\n",
            "1/1 - 0s - loss: 0.1054 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 195/200\n",
            "1/1 - 0s - loss: 0.1036 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 196/200\n",
            "1/1 - 0s - loss: 0.1019 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 197/200\n",
            "1/1 - 0s - loss: 0.1002 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 198/200\n",
            "1/1 - 0s - loss: 0.0985 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 199/200\n",
            "1/1 - 0s - loss: 0.0969 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 200/200\n",
            "1/1 - 0s - loss: 0.0953 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcbde92ed90>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_generation(model, tokenizer, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
        "    init_word = current_word\n",
        "    sentence = ''\n",
        "\n",
        "    # n번 반복\n",
        "    for _ in range(n):\n",
        "        # 현재 단어에 대한 정수 인코딩과 패딩\n",
        "        encoded = tokenizer.texts_to_sequences([current_word])[0]\n",
        "        encoded = pad_sequences([encoded], maxlen=5, padding='pre')\n",
        "        # 입력한 X(현재 단어)에 대해서 Y를 예측하고 Y(예측한 단어)를 result에 저장.\n",
        "        result = model.predict(encoded, verbose=0)\n",
        "        result = np.argmax(result, axis=1)\n",
        "\n",
        "        for word, index in tokenizer.word_index.items(): \n",
        "            # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면 break\n",
        "            if index == result:\n",
        "                break\n",
        "\n",
        "        # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
        "        current_word = current_word + ' '  + word\n",
        "\n",
        "        # 예측 단어를 문장에 저장\n",
        "        sentence = sentence + ' ' + word\n",
        "\n",
        "    sentence = init_word + sentence\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "kvRBlkSscEhj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence_generation(model, tokenizer, '경마장에', 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98ynJPP4cOGk",
        "outputId": "86084db2-432c-4919-ffc3-1e37a39def24"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "경마장에 있는 말이 뛰고 있다\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence_generation(model1, tokenizer, '경마장에', 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBa-fJVTcqho",
        "outputId": "3ee8444e-54df-4b72-d4de-1e586b5dd34c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "경마장에 있는 말이 뛰고 있다\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence_generation(model, tokenizer, '그의', 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YxunTggcQcE",
        "outputId": "d917c376-bc77-4aac-9621-c30f0d7e62a1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "그의 말이 법이다\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence_generation(model1, tokenizer, '그의', 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F-tCKaXcwUC",
        "outputId": "12085b46-9242-461f-fc36-b2d00c8325cf"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "그의 말이 법이다\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence_generation(model, tokenizer, '가는', 5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9W4wggzcSRj",
        "outputId": "e1280211-438e-4148-e5a1-d2947050d68e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "가는 말이 고와야 오는 말이 곱다\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence_generation(model1, tokenizer, '가는', 5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1mu7jcicyLV",
        "outputId": "a292378e-ecf4-49d0-c7c8-7397bada5848"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "가는 말이 고와야 오는 말이 곱다\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1D CNN으로 IMDB 분류하기"
      ],
      "metadata": {
        "id": "VMIaCwgOc9ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 데이터에 대한 전처리"
      ],
      "metadata": {
        "id": "5mqKeASUdCLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import datasets\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "FKC3UOVicT3i"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "최대 10,000개의 단어만을 허용하여 데이터를 로드한다."
      ],
      "metadata": {
        "id": "riYtYRiedFZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "(X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(num_words=vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IScQryZydEc0",
        "outputId": "f1df972b-9105-4b17-b747-5e286e7d6939"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuyQvMR3dMLK",
        "outputId": "0145f02d-c408-485c-fe8c-58da8b63817b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32])\n",
            " list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 8255, 2, 349, 2637, 148, 605, 2, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95])\n",
            " list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])\n",
            " list([1, 4, 2, 2, 33, 2804, 4, 2040, 432, 111, 153, 103, 4, 1494, 13, 70, 131, 67, 11, 61, 2, 744, 35, 3715, 761, 61, 5766, 452, 9214, 4, 985, 7, 2, 59, 166, 4, 105, 216, 1239, 41, 1797, 9, 15, 7, 35, 744, 2413, 31, 8, 4, 687, 23, 4, 2, 7339, 6, 3693, 42, 38, 39, 121, 59, 456, 10, 10, 7, 265, 12, 575, 111, 153, 159, 59, 16, 1447, 21, 25, 586, 482, 39, 4, 96, 59, 716, 12, 4, 172, 65, 9, 579, 11, 6004, 4, 1615, 5, 2, 7, 5168, 17, 13, 7064, 12, 19, 6, 464, 31, 314, 11, 2, 6, 719, 605, 11, 8, 202, 27, 310, 4, 3772, 3501, 8, 2722, 58, 10, 10, 537, 2116, 180, 40, 14, 413, 173, 7, 263, 112, 37, 152, 377, 4, 537, 263, 846, 579, 178, 54, 75, 71, 476, 36, 413, 263, 2504, 182, 5, 17, 75, 2306, 922, 36, 279, 131, 2895, 17, 2867, 42, 17, 35, 921, 2, 192, 5, 1219, 3890, 19, 2, 217, 4122, 1710, 537, 2, 1236, 5, 736, 10, 10, 61, 403, 9, 2, 40, 61, 4494, 5, 27, 4494, 159, 90, 263, 2311, 4319, 309, 8, 178, 5, 82, 4319, 4, 65, 15, 9225, 145, 143, 5122, 12, 7039, 537, 746, 537, 537, 15, 7979, 4, 2, 594, 7, 5168, 94, 9096, 3987, 2, 11, 2, 4, 538, 7, 1795, 246, 2, 9, 2, 11, 635, 14, 9, 51, 408, 12, 94, 318, 1382, 12, 47, 6, 2683, 936, 5, 6307, 2, 19, 49, 7, 4, 1885, 2, 1118, 25, 80, 126, 842, 10, 10, 2, 2, 4726, 27, 4494, 11, 1550, 3633, 159, 27, 341, 29, 2733, 19, 4185, 173, 7, 90, 2, 8, 30, 11, 4, 1784, 86, 1117, 8, 3261, 46, 11, 2, 21, 29, 9, 2841, 23, 4, 1010, 2, 793, 6, 2, 1386, 1830, 10, 10, 246, 50, 9, 6, 2750, 1944, 746, 90, 29, 2, 8, 124, 4, 882, 4, 882, 496, 27, 2, 2213, 537, 121, 127, 1219, 130, 5, 29, 494, 8, 124, 4, 882, 496, 4, 341, 7, 27, 846, 10, 10, 29, 9, 1906, 8, 97, 6, 236, 2, 1311, 8, 4, 2, 7, 31, 7, 2, 91, 2, 3987, 70, 4, 882, 30, 579, 42, 9, 12, 32, 11, 537, 10, 10, 11, 14, 65, 44, 537, 75, 2, 1775, 3353, 2, 1846, 4, 2, 7, 154, 5, 4, 518, 53, 2, 2, 7, 3211, 882, 11, 399, 38, 75, 257, 3807, 19, 2, 17, 29, 456, 4, 65, 7, 27, 205, 113, 10, 10, 2, 4, 2, 2, 9, 242, 4, 91, 1202, 2, 5, 2070, 307, 22, 7, 5168, 126, 93, 40, 2, 13, 188, 1076, 3222, 19, 4, 2, 7, 2348, 537, 23, 53, 537, 21, 82, 40, 2, 13, 2, 14, 280, 13, 219, 4, 2, 431, 758, 859, 4, 953, 1052, 2, 7, 5991, 5, 94, 40, 25, 238, 60, 2, 4, 2, 804, 2, 7, 4, 9941, 132, 8, 67, 6, 22, 15, 9, 283, 8, 5168, 14, 31, 9, 242, 955, 48, 25, 279, 2, 23, 12, 1685, 195, 25, 238, 60, 796, 2, 4, 671, 7, 2804, 5, 4, 559, 154, 888, 7, 726, 50, 26, 49, 7008, 15, 566, 30, 579, 21, 64, 2574])\n",
            " list([1, 249, 1323, 7, 61, 113, 10, 10, 13, 1637, 14, 20, 56, 33, 2401, 18, 457, 88, 13, 2626, 1400, 45, 3171, 13, 70, 79, 49, 706, 919, 13, 16, 355, 340, 355, 1696, 96, 143, 4, 22, 32, 289, 7, 61, 369, 71, 2359, 5, 13, 16, 131, 2073, 249, 114, 249, 229, 249, 20, 13, 28, 126, 110, 13, 473, 8, 569, 61, 419, 56, 429, 6, 1513, 18, 35, 534, 95, 474, 570, 5, 25, 124, 138, 88, 12, 421, 1543, 52, 725, 6397, 61, 419, 11, 13, 1571, 15, 1543, 20, 11, 4, 2, 5, 296, 12, 3524, 5, 15, 421, 128, 74, 233, 334, 207, 126, 224, 12, 562, 298, 2167, 1272, 7, 2601, 5, 516, 988, 43, 8, 79, 120, 15, 595, 13, 784, 25, 3171, 18, 165, 170, 143, 19, 14, 5, 7224, 6, 226, 251, 7, 61, 113])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "각 샘플의 길이가 긴 관계로 출력 시 중간 내용은 중략하였습니다. 각 샘플은 이미 정수 인코딩까지 전처리가 된 상태입니다. 하지만 각 샘플들의 길이는 서로 다르죠? 패딩을 진행하여 모든 샘플들의 길이를 200으로 맞춥니다."
      ],
      "metadata": {
        "id": "WLwThcyqdT2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 200\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)"
      ],
      "metadata": {
        "id": "-a0SENDcdOk4"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "패딩이 되었는지 크기(shape)를 확인해봅시다."
      ],
      "metadata": {
        "id": "w-s2TDzddW3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('X_train의 크기(shape) :',X_train.shape)\n",
        "print('X_test의 크기(shape) :',X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yotrvv3dVi9",
        "outputId": "c8ecf9d4-aae6-40f9-967c-a66d0cab7e9a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train의 크기(shape) : (25000, 200)\n",
            "X_test의 크기(shape) : (25000, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련 데이터, 테스트 데이터 각 25,000 샘플이 전부 길이 200을 가지는 것을 확인할 수 있습니다. y_train도 출력해봅시다."
      ],
      "metadata": {
        "id": "XZZ0ZYsqdZSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfyhIzxxdYHV",
        "outputId": "b8015756-905e-4592-b3bf-4f31a2756925"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 0 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 1D CNN으로 IMDB 리뷰 분류하기"
      ],
      "metadata": {
        "id": "RYpSuoaWdbc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "embedding_dim = 256 # 임베딩 벡터의 차원\n",
        "dropout_ratio = 0.3 # 드롭아웃 비율\n",
        "num_filters = 256 # 커널의 수\n",
        "kernel_size = 3 # 커널의 크기\n",
        "hidden_units = 128 # 뉴런의 수\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(Dropout(dropout_ratio))\n",
        "model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(hidden_units, activation='relu'))\n",
        "model.add(Dropout(dropout_ratio))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), callbacks=[es, mc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UZz75lmdakk",
        "outputId": "96ceb03e-6e38-4d79-8d29-24e6f023c29a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "782/782 [==============================] - ETA: 0s - loss: 0.4024 - acc: 0.8048\n",
            "Epoch 1: val_acc improved from -inf to 0.87220, saving model to best_model.h5\n",
            "782/782 [==============================] - 162s 206ms/step - loss: 0.4024 - acc: 0.8048 - val_loss: 0.2960 - val_acc: 0.8722\n",
            "Epoch 2/20\n",
            "782/782 [==============================] - ETA: 0s - loss: 0.2061 - acc: 0.9194\n",
            "Epoch 2: val_acc improved from 0.87220 to 0.89176, saving model to best_model.h5\n",
            "782/782 [==============================] - 162s 207ms/step - loss: 0.2061 - acc: 0.9194 - val_loss: 0.2636 - val_acc: 0.8918\n",
            "Epoch 3/20\n",
            "782/782 [==============================] - ETA: 0s - loss: 0.0927 - acc: 0.9694\n",
            "Epoch 3: val_acc did not improve from 0.89176\n",
            "782/782 [==============================] - 161s 206ms/step - loss: 0.0927 - acc: 0.9694 - val_loss: 0.3407 - val_acc: 0.8826\n",
            "Epoch 4/20\n",
            "782/782 [==============================] - ETA: 0s - loss: 0.0419 - acc: 0.9861\n",
            "Epoch 4: val_acc did not improve from 0.89176\n",
            "782/782 [==============================] - 161s 206ms/step - loss: 0.0419 - acc: 0.9861 - val_loss: 0.4240 - val_acc: 0.8802\n",
            "Epoch 5/20\n",
            "782/782 [==============================] - ETA: 0s - loss: 0.0232 - acc: 0.9916\n",
            "Epoch 5: val_acc did not improve from 0.89176\n",
            "782/782 [==============================] - 161s 206ms/step - loss: 0.0232 - acc: 0.9916 - val_loss: 0.5011 - val_acc: 0.8706\n",
            "Epoch 5: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = load_model('best_model.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rkjt_gcdgJF",
        "outputId": "b1d29cc5-af46-43c0-94f4-5a5624731f80"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 31s 40ms/step - loss: 0.2636 - acc: 0.8918\n",
            "\n",
            " 테스트 정확도: 0.8918\n"
          ]
        }
      ]
    }
  ]
}